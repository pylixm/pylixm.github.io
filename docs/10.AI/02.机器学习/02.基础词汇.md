---
title: AI 基础词汇
permalink: /pages/a2c2f3/
  - 机器学习
tags:
  - null
createTime: 2023/09/08 17:36:02
---


- 人工智能（Artificial Intelligence，简称 AI）：是一种计算机技术，旨在模拟人类智能，实现自主学习、推理、判断和行动的能力。

- 机器学习（Machine Learning，简称 ML）：是一种人工智能技术，利用数据和统计方法，让计算机自动学习和提高性能，无需显式编程。

- 深度学习（Deep Learning）：是一种机器学习技术，使用深度神经网络模型进行大规模非线性变换和特征提取，从而实现高性能的模式识别和预测。

- 自然语言处理（Natural Language Processing，简称 NLP）：是一种人工智能技术，使计算机能够理解和生成人类语言，包括语音识别、文本分类、情感分析等任务。

- 计算机视觉（Computer Vision，简称 CV）：是一种人工智能技术，使计算机能够理解和分析图像和视频，包括目标检测、图像识别、人脸识别等任务。

- 强化学习（Reinforcement Learning）：是一种机器学习技术，通过让智能体与环境互动，学习如何做出最优决策，以最大化奖励信号。

- 数据挖掘（Data Mining）：是一种从大量数据中发现模式和关系的技术，包括分类、聚类、关联规则挖掘等任务。

- 人机交互（Human-Computer Interaction，简称 HCI）：是一种研究人与计算机之间交互的学科，旨在改进用户体验和提高计算机系统的易用性。

- 模型评估（Model Evaluation）：是一种评估机器学习模型性能的技术，包括精度、召回率、F1 值、AUC 等指标。

- 模型优化（Model Optimization）：是一种提高机器学习模型性能的技术，包括特征工程、模型选择、超参数调优等方法。

- 人工智能伦理（AI Ethics）：是一种研究人工智能应用对社会和个人价值观的影响，以及如何确保人工智能符合伦理和社会价值的学科。

- 自动化（Automation）：是一种自动化执行任务和流程的技术，以提高效率、减少错误和成本。在 AI 领域，自动化通常指自动

## Sota
Sota实际上就是State of the arts 的缩写，指的是在某一个领域做的Performance最好的model，一般就是指在一些benchmark的数据集上跑分非常高的那些模型。

SOTA model：并不是特指某个具体的模型，而是指在该项研究任务中，目前最好/最先进的模型。
SOTA result：指的是在该项研究任务中，目前最好的模型的结果/性能/表现。

## 非端到端模型
传统机器学习的流程往往由多个独立的模块组成，比如在一个典型的自然语言处理（Natural Language Processing）问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端的。

## 端到端模型
从输入端到输出端会得到一个预测结果，将预测结果和真实结果进行比较得到误差，将误差反向传播到网络的各个层之中，调整网络的权重和参数直到模型收敛或者达到预期的效果为止，中间所有的操作都包含在神经网络内部，不再分成多个模块处理。由原始数据输入，到结果输出，从输入端到输出端，中间的神经网络自成一体（也可以当做黑盒子看待），这是端到端的。

## Benchmark、Baseline
Benchmark和baseline都是指最基础的比较对象。你论文的motivation来自于想超越现有的baseline/benchmark，你的实验数据都需要以baseline/benckmark为基准来判断是否有提高。唯一的区别就是baseline讲究一套方法，而benchmark更偏向于一个目前最高的指标，比如precision，recall等等可量化的指标。举个例子，NLP任务中BERT是目前的SOTA，你有idea可以超过BERT。那在论文中的实验部分你的方法需要比较的baseline就是BERT，而需要比较的benchmark就是BERT具体的各项指标。

## 并发、并行、串行
我中午12：00开始吃饭，吃到一半，女朋友打来一个电话，我需要等到我吃完饭才可以接电话，这说明我不支持并行与并发，我的运作方式属于串行，串行有一个执行单元（只有一个执行任务单元的cpu核）。
我中午12:00开始吃饭，吃到一半，女朋友打来一个电话，我可以接起电话，跟女朋友打完电话继续吃饭，这说明我支持并发与串行，
我中午12:00开始吃饭，吃到一半，女朋友打来一个电话，我可以一边接电话一边吃饭，这说明我支持并行与并发。（并行有多个任务执行单元，多个任务可以同时执行）
所谓并发，是指我有没有同时处理多个任务的能力，不一定要同时。

## 迁移学习
迁移学习通俗来讲，就是运用已有的知识来学习新的知识，核心是找到已有知识和新知识之间的相似性，用成语来说就是举一反三。由于直接对目标域从头开始学习成本太高，我们故而转向运用已有的相关知识来辅助尽快地学习新知识。比如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#；已经学会英语，就可以类比着来学习法语；等等。世间万事万物皆有共性，如何合理地找寻它们之间的相似性，进而利用这个桥梁来帮助学习新知识，是迁移学习的核心问题。

## 微调
微调其实讲的是利用原有模型参数（“知识”）初始化现有模型，在此基础上继续train自己的model（“再加工”）。说人话就是把现成的模型略加修改然后再作少量training，主要用于样本数量不足的情形。

## 进程、线程
一个进程包括多个线程。
不同进程之间数据很难共享。
同一个进程下的不同线程数据很容易共享。
进程比线程消耗更多计算机资源。
进程之间互不影响，但是一个进程挂掉，他所在的整个进程都会挂掉。
进程可以拓展到多机，适合多核与分布式。
进程使用的内存地址可以限定使用量。

## 监督学习
是使用足够多的带有label的数据集来训练模型，数据集中的每个样本都带有人工标注的label。通俗理解就是，模型在学习的过程中，“老师”指导模型应该向哪个方向学习或调整。

## 非监督学习
是指训练模型用的数据没有人工标注的标签信息，通俗理解就是在“没有老师指导”的情况下，靠“学生”自己通过不断地探索，对知识进行归纳和总结，尝试发现数据中的内在规律或特征，来对训练数据打标签。

## 半监督学习
是在只能获取少量的带label的数据，但是可以获取大量的的数据的情况下训练模型，让学习器不依赖于外界交互，自动地利用未标记样本来提升学习性能，半监督学习是监督学习和非监督学习的相结合的一种学习方法。

## 泛化（Generalization）
模型的泛化能力通俗易懂的说就是模型在测试集（其中的数据模型以前没有见过）中的表现，也就是模型举一反三的能力，但是这些数据必须满足与iid（独立同分布）并在同一个分布中。
举个例子：一张图片模型之前没有见过，但是这张图片与TrainDataSet在同一分布，并满足iid，模型可以很好的预测这张图，这就是模型的泛化，在测试集中，模型预测新数据的准确率越高，就可以说是模型的泛化能力越好。

## 正则化（Regularization）
正则化即为对学习算法的修改，旨在减少泛化误差而不是训练误差。正则化的策略包括：

- 约束和惩罚被设计为编码特定类型的先验知识。
- 偏好简单模型。
- 其他形式的正则化，如：集成的方法，即结合多个假说解释训练数据。

## 吞吐量
首先在书面解释时，速率是额定或标称的，但是实际传输时，其实不稳定的，吞吐量就是取平均值。假设你从学校骑电动车回家，这条公路限速80km/h，这就可以理解成“带宽”，也就是“最高传输速率”。所骑电动车厂家宣称最高时速30km/h，这可以理解成“速率”，也就是“额定速率或标称速率”。但是你不可能全程以30km/h速度行驶，可能会碰到红灯或者堵车，这时你的速度就会放慢了，这条路的长度除以你行驶时间所得平均行驶速度，就可以理解成“吞吐量”。

```python
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l
from torch import nn
%matplotlib inline
d2l.use_svg_display()
```