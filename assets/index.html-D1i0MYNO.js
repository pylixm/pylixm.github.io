import{_ as t,c as r,a,o}from"./app-DdES4ywf.js";const p="/imgs/k8s/monitor-arch.png",n={};function i(s,e){return o(),r("div",null,[...e[0]||(e[0]=[a('<p>在讲方案之前，先说下应用场景。</p><p>目前我司有部分云厂商的云服务器且是不同厂商，也有托管机房的物理设备，笼统来说这已经算是一个混合云的场景了。我们服务部署有直接在云服务器部署的，也有在容器平台Rancher，各服务之间尽量做到了不跨厂商机房交互，业务逻辑上各机房互为备份，有些必要的交互，没专线只能走公网，做了ip和防火墙的限制。</p><p>针对这种简易混合云的业务场景，如何做基础和业务监控呢？下面我来简单说下我们的方案，希望可以抛砖引玉，共同交流。</p><h2 id="整体架构" tabindex="-1"><a class="header-anchor" href="#整体架构"><span>整体架构</span></a></h2><p>一直比较中意 Prometheus 的指标+标签的数据存储和查询方式，想把主机的监控数据加入进去，但是需要部署 exporter、docker，增加了好多组件。如果自己收集又需要一个数据转换的网关式的服务，极为不便。</p><p>之前有在关注夜莺，V4.0+ 加入了 Prometheus 的数据源，但是整体使用和组件部署还是偏繁琐。V5 版本的发布，如获至宝，非常契合我们的需求场景。我们的混合云监控，便是围绕 夜莺v5 构建的。整体架构图如下：</p><p><img src="'+p+'" alt=""></p><p>简要说明：</p><ul><li>local-cluster/prod-cluster: 两个 Rancher 生态的 K3S 集群；</li><li>Prometheus: 其中有两个 Prometheus 集群，分别收集所属集群的容器平台相关监控；</li><li>VictoriaMetric: 集群部署在 local 集群，和业务数据分离，避免负载相互影响；</li><li>server-local: n9e server 负责针对 local 集群的报警规则配置、报警、数据查询等；</li><li>server-prod: n9e server 负责针对 prod 集群的报警规则配置、报警、数据查询、主机监控上报等；</li><li>webapi: n9e webapi 前端页面，主要功能规则配置（报警和故障自愈）、数据查询、告警查看、业务权限隔离等；</li></ul><p>下边我分别从监控的几个重要组件说下我们的选型和思考。</p><h2 id="采集器" tabindex="-1"><a class="header-anchor" href="#采集器"><span>采集器</span></a></h2><p>业界有很多采集器，我司需要采集的场景主要分两种，容器和主机，针对容器我们采用 Prometheus 生态的各种 exporter，针对主机我们采用了夜莺团队推荐的 <a href="https://docs.influxdata.com/telegraf/v1.23/" target="_blank" rel="noopener noreferrer">telegraf</a>（后来夜莺团队开发了自己的<a href="https://n9e.github.io/docs/agent/categraf/" target="_blank" rel="noopener noreferrer">categraf</a>），telegraf 各种采集也是插件化的，生态异常丰富，基本覆盖了cpu、内存、disk、process等基础监控和各种中间件监控。除了自带的核心插件外，我们可以编写自己的采集逻辑，通过telegraf 的 exec 插件来实现自定义采集。</p><p>容器这块的采集器Prometheus生态已成为一种事实上的标准。主机采集这块选择 telegraf 主要是基于如下几点：</p><ul><li>telegraf 二进制文件，无依赖库，占用资源相对小；</li><li>go开发，契合团队技术栈，有问题可二次开发；</li><li>丰富的插件，可自定义插件，这样我们可以把之前很多 openfalcon 的插件移植过来；</li></ul><p>后期夜莺团队开源的 categraf，很好的继承了 telegraf 的以上有点，并做了大量的优化，推荐试用。</p><p>针对采集上报数据的整个链路，n9e-server 起到了 push-gateway 的作用，让整体架构具有了 push 的模型能力。使整体的上报功能更丰富，适应场景更广。</p><h2 id="存储" tabindex="-1"><a class="header-anchor" href="#存储"><span>存储</span></a></h2><p>存储这块，PromQL 类的存储和查询是我们想要的，Prometheus 又是容器这块的标配，我们围绕 Promethues 做了高可用持久化方案的调研，针对目前比较流行的 Thanos 架构、Cortex 架构、VictoriaMetric 架构进行了对比，最终选用了 VictoriaMetric。具体的对比总结可看我之前的文章 <a href="https://pylixm.top/posts/2022-03-02-k3s-monitoring.html#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8" target="_blank" rel="noopener noreferrer">k3s 监控方案调研</a></p><p><a href="https://docs.victoriametrics.com/" target="_blank" rel="noopener noreferrer">VictoriaMetric</a> 的选用主要基于以下几点：</p><ul><li>组件架构建链，不依赖第三方组件，部署维护简单方便；</li><li>完全兼容 PromQL 语法，增加了自己特有的语法；</li><li>夜莺系统的完全兼容；</li></ul><h2 id="报警" tabindex="-1"><a class="header-anchor" href="#报警"><span>报警</span></a></h2><p>报警这块应该是我们使用夜莺的另一大动力，它完全可以替代 Prometheus 生态的 Alertmanager, 将各种 Rule 直接存储在数据库，理解和配置更加方便。 最新版本，夜莺使用 go 重新了之前的报警脚本，内置了邮件、钉钉、企业微信的报警通道。同时保留了之前报警脚本的功能，以支持脚本的自定义。整体来说完全满足需求，特殊需求完全可自己定制。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>基于夜莺的这套系统部署，完美的满足了我司容器和主机场景下的混合云监控。</p>',24)])])}const c=t(n,[["render",i]]),m=JSON.parse('{"path":"/pages/c76af1/","title":"基于n9e的 Rancher容器平台和主机混合服务架构监控方案","lang":"zh-CN","frontmatter":{"title":"基于n9e的 Rancher容器平台和主机混合服务架构监控方案","description":"基于n9e的 Rancher容器平台和主机混合服务架构监控方案。","toc":true,"tags":["Kubernetes","k3s","监控"],"draft":false,"permalink":"/pages/c76af1/","createTime":"2023/09/08 17:36:02","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"基于n9e的 Rancher容器平台和主机混合服务架构监控方案\\",\\"image\\":[\\"https://pylixm.top/imgs/k8s/monitor-arch.png\\"],\\"dateModified\\":\\"2025-09-30T08:57:49.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://pylixm.top/pages/c76af1/"}],["meta",{"property":"og:site_name","content":"底层逻辑"}],["meta",{"property":"og:title","content":"基于n9e的 Rancher容器平台和主机混合服务架构监控方案"}],["meta",{"property":"og:description","content":"基于n9e的 Rancher容器平台和主机混合服务架构监控方案。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://pylixm.top/imgs/k8s/monitor-arch.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-30T08:57:49.000Z"}],["meta",{"property":"article:tag","content":"监控"}],["meta",{"property":"article:tag","content":"k3s"}],["meta",{"property":"article:tag","content":"Kubernetes"}],["meta",{"property":"article:modified_time","content":"2025-09-30T08:57:49.000Z"}]]},"readingTime":{"minutes":4.32,"words":1297},"git":{"createdTime":1759222669000,"updatedTime":1759222669000,"contributors":[{"name":"pylixm","username":"pylixm","email":"pyli.xm@gmail.com","commits":1,"avatar":"https://avatars.githubusercontent.com/pylixm?v=4","url":"https://github.com/pylixm"}]},"filePathRelative":"04.云原生/02.k3s/03.k3s-n9e-monitor.md","headers":[],"categoryList":[{"id":"e467f5","sort":4,"name":"云原生"},{"id":"e868ae","sort":2,"name":"k3s"}]}');export{c as comp,m as data};
